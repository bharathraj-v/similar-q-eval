# Single Agent Evaluation - Prompts Configuration
# This file contains the system prompt and user message template for single agent evaluation

agent:
  single_evaluator:
    system_prompt: |
      You are an academic expert at analyzing and solving problems in the field of maths, physics and chemistry.
      Here, you are given a question and you must evaluate a similar question from the dataset and return the results.
      The image descriptions after the question given are also part of the question, make sure to include them.
      You must assess how well the similar question represents the input question across the following dimensions:
      
      1. Conceptual Similarity: Do they test the same underlying concepts/principles?
      2. Structural Similarity: Are the problem structures analogous?
      3. Difficulty Alignment: Is the difficulty level appropriate?
      4. Solution Approach Transferability: Can the solution method be meaningfully applied?
      
      You must return a JSON object with the following keys:
      similar_question: The similar question
      solution_approach: The solution approach
      conceptual_similarity_score: A score between 0 and 100
      structural_similarity_score: A score between 0 and 100
      difficulty_alignment_score: A score between 0 and 100
      solution_approach_transferability_score: A score between 0 and 100
      total_score: A score between 0 and 100 which is the average of all the individual scores
      notes: A comprehensive note explaining the complete evaluation of the relevance of the similar question
    
    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate the similar question across all four dimensions and return comprehensive results.

evaluation_config:
  scoring:
    min_score: 0
    max_score: 100
    total_max_score: 100 
  
  validation:
    min_text_length: 10
    max_question_length: 5000
    max_solution_length: 10000
  
  descriptions:
    conceptual_similarity: "Evaluates if two questions test the same underlying concepts and principles"
    structural_similarity: "Evaluates if the problem structures are analogous, considering information given and problem setup"
    difficulty_alignment: "Evaluates if two questions have appropriate and similar difficulty levels"
    approach_transferability: "Evaluates if the solution method can be meaningfully applied to solve another question"

messages:
  startup:
    success: "Evaluation API started successfully"
    llm_init: "LLM initialized successfully"
    agent_init: "Agent initialized successfully"
    failure: "Startup failed: {error}"
  
  evaluation:
    start: "Starting evaluation for request {request_id}"
    success: "Evaluation completed successfully in {processing_time}ms"
    failure: "Evaluation failed: {error}"
  
  errors:
    service_unavailable: "Service not ready - agent not initialized"
    validation_error: "Validation error: {error}"
    internal_error: "Internal server error"
    missing_api_key: "GEMINI_API_KEY environment variable is required"

# API metadata
api:
  title: "Similar Questions Evaluation API (Single Agent)"
  description: "A production-ready API for evaluating the similarity and transferability of academic questions using a single agent"
  version: "1.0.0"
  
  endpoints:
    health:
      description: "Health check endpoint to verify service status"
    evaluate:
      description: |
        Evaluate the similarity and transferability between an original question and a similar question.
        
        This endpoint performs a comprehensive evaluation across four dimensions:
        - Conceptual similarity: Tests underlying concepts/principles
        - Structural similarity: Analogous problem structures and setups
        - Difficulty alignment: Appropriate and similar difficulty levels
        - Solution approach transferability: Method applicability across questions
        
        Returns scores (0-100) for each dimension plus comprehensive analysis notes.