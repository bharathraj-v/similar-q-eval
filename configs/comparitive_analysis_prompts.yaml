# Comparative Analysis - Prompts and Configuration

agent:
  comparitive_analyzer:
    system_prompt: |
      You are an academic expert at solving problems in the field of maths, physics and chemistry. 
      You are tasked with evaluating different answers to a question and you score them based on their accuracy and completeness.
      You receive the following information from the user.

      The answers you receive are of two categories - sim answers and non-sim answers, You must score both of the answers 

      Here is everything you receive:
      question: The question to be solved
      sim_answer_explanation: A step by step explanation of the solution approach to the question
      sim_answer_final_answer: The final answer to the question
      no_sim_answer_explanation: A step by step explanation of the solution approach to the question
      no_sim_answer_final_answer: The final answer to the question

      You must respond with an object with the following keys:
      sim_answer_score: A score between 0 and 100 for the sim answer
      no_sim_answer_score: A score between 0 and 100 for the no sim answer
      notes: A note explaining the comparitive analysis between both answers explaining which is better and why

      Make sure to thoroughly analyze both questions on their correctness, accuracy, completeleness

    user_message_template: |
      Here is a question: {question}

      sim_answer_explanation: {sim_answer_explanation}
      sim_answer_final_answer: {sim_answer_final_answer}

      no_sim_answer_explanation: {no_sim_answer_explanation}
      no_sim_answer_final_answer: {no_sim_answer_final_answer}

      Please respond with the comparitive analysis.

messages:
  startup:
    success: "Comparative Analysis API started successfully."
    llm_init: "LLM initialized successfully."
    agent_init: "Comparison Agent initialized successfully."
    failure: "Critical startup failure: {error}"
  
  analysis:
    start: "Starting comparative analysis for request {request_id}."
    success: "Request {request_id}: Analysis completed successfully in {processing_time}ms."
    failure: "Analysis failed for request {request_id}: {error}"
  
  errors:
    service_unavailable: "Service is not ready. The comparison agent has not been initialized."
    validation_error: "Invalid request: {error}"
    internal_error: "An unexpected internal server error occurred."
    missing_api_key: "GEMINI_API_KEY environment variable is not set or is empty."

# API metadata
api:
  title: "Solution Comparative Analysis API"
  description: "A production-ready API that performs a comparative analysis between two different solutions for the same academic question."
  version: "1.0.0"
  
  endpoints:
    health:
      description: "Health check endpoint to verify that the service is running and configured correctly."
    analyse:
      description: |
        Provides a comparative analysis of two solutions for a given question.
        
        This endpoint takes a question and two complete solutions (one derived with the help of similar questions, one without) and returns:
        - A score (0-100) for the 'sim' answer.
        - A score (0-100) for the 'no_sim' answer.
        - Detailed notes explaining the rationale behind the scores and which solution is better.
