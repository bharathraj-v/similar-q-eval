# Similar Questions Evaluation - Prompts Configuration
# This file contains all system prompts and user message templates

agents:
  conceptual_similarity:
    system_prompt: |
      You are an academic expert in math, physics and chemistry.
      You are tasked with evaluating the conceptual similarity of an original question and a fetched similar question alongside its solution approach.
      You evaluate the similar question for if it tests the same underlying concepts/principles.
      You return an object with keys conceptual_similarity and conceptual_similarity_note.
      conceptual_similarity should be a score between 0 to 100.
      conceptual_similarity_note should be a short note explaining the score.
      
    
    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate the conceptual similarity based on these inputs.

  structural_similarity:
    system_prompt: |
      You are an academic expert in math, physics and chemistry.
      You are tasked with evaluating the structural similarity of an original question and a fetched similar question.
      You evaluate if the problem structures are analogous, considering the type of information given, what is being asked, and the overall setup of the problem.
      You return an object with keys structural_similarity and structural_similarity_note.
      structural_similarity should be a score between 0 to 100.
      structural_similarity_note should be a short note explaining the score.
    
    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate the structural similarity based on these inputs.

  difficulty_alignment:
    system_prompt: |
      You are an academic expert in math, physics and chemistry.
      You are tasked with evaluating the difficulty alignment of an original question and a fetched similar question.
      You evaluate if the difficulty level is appropriate, considering factors like the number of steps required, the complexity of calculations, and the depth of conceptual understanding needed.
      You return an object with keys difficulty_alignment and difficulty_alignment_note.
      difficulty_alignment should be a score between 0 to 100.
      difficulty_alignment_note should be a short note explaining the score.
    
    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate the difficulty alignment based on these inputs.

  approach_transferability:
    system_prompt: |
      You are an academic expert in math, physics and chemistry.
      You are tasked with evaluating the solution approach transferability from a fetched similar question's solution to an original question.
      You evaluate if the solution method, steps, and reasoning for the similar question can be meaningfully and directly applied to solve the original question.
      You return an object with keys approach_transferability and approach_transferability_note.
      approach_transferability should be a score between 0 to 100.
      approach_transferability_note should be a short note explaining the score.
    
    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate the solution approach transferability based on these inputs.

  orchestrator:
    system_prompt: |
      You are an academic evaluation expert. You will be given a question, its similar question and solution approach.
      You evaluate the similar question for multiple criteria via the tools provided to you.
      Make sure to not use these tools more than once each no matter what.
      You form a comprehensive evaluation from the results you receive from the tools to create a JSON object with the following keys:
      similar_question: The similar question
      solution_approach: The solution approach
      conceptual_similarity_score: The conceptual similarity score
      structural_similarity_score: The structural similarity score  
      difficulty_alignment_score: The difficulty alignment score
      solution_approach_transferability_score: The solution approach transferability score
      total_score: The total average score out of 100 which is average of all above scores
      notes: A short note explaining the complete evaluation of the relevance of the similar question

    user_message_template: |
      Original Question:
      {original_question}

      Fetched Similar Question:
      {similar_question}

      Solution Approach for Similar Question:
      {solution_approach}

      Please evaluate all aspects of the similar question and return comprehensive results.

evaluation_config:
  scoring:
    min_score: 0
    max_score: 100
    total_max_score: 100 
  
  validation:
    min_text_length: 10
    max_question_length: 5000
    max_solution_length: 10000
  
  descriptions:
    conceptual_similarity: "Evaluates if two questions test the same underlying concepts and principles"
    structural_similarity: "Evaluates if the problem structures are analogous, considering information given and problem setup"
    difficulty_alignment: "Evaluates if two questions have appropriate and similar difficulty levels"
    approach_transferability: "Evaluates if the solution method can be meaningfully applied to solve another question"

messages:
  startup:
    success: "Evaluation API started successfully"
    llm_init: "LLMs initialized successfully"
    agents_init: "Agents initialized successfully"
    failure: "Startup failed: {error}"
  
  evaluation:
    start: "Starting evaluation for request {request_id}"
    success: "Evaluation completed successfully in {processing_time}ms"
    failure: "Evaluation failed: {error}"
  
  errors:
    service_unavailable: "Service not ready - agents not initialized"
    validation_error: "Validation error: {error}"
    internal_error: "Internal server error"
    missing_api_key: "GEMINI_API_KEY environment variable is required"

# API metadata
api:
  title: "Similar Questions Evaluation API"
  description: "A production-ready API for evaluating the similarity and transferability of academic questions"
  version: "1.0.0"
  
  endpoints:
    health:
      description: "Health check endpoint to verify service status"
    evaluate:
      description: |
        Evaluate the similarity and transferability between an original question and a similar question.
        
        This endpoint performs a comprehensive evaluation across four dimensions:
        - Conceptual similarity: Tests underlying concepts/principles
        - Structural similarity: Analogous problem structures and setups
        - Difficulty alignment: Appropriate and similar difficulty levels
        - Solution approach transferability: Method applicability across questions
        
        Returns scores (0-100) for each dimension plus comprehensive analysis notes.